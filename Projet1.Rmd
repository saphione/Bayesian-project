---
title: "Projet1_Bayesian"
author: "Lorazo, M. De Koninck, A"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Bayesian's project

Article : Patches of Bare Ground as a Staple Commodity for Declining Ground-Foraging Insectivorous Farmland Birds

## 1. Loading and exploring the data

### Loading packages

```{r Loading packages, include=FALSE}
library(readr)
library(R2jags)
library(ggplot2)
```

### Loading dataset

```{r Loading dataset, include=FALSE}
hoopoe <- read_delim("C:/Users/Saphione/OneDrive/Fac/M2/Cours/Bayésien/Projet/projet1/hoopoe.csv", delim = ";", escape_double = FALSE, trim_ws = TRUE)
hoopoe <- hoopoe[,c(1:3,5:7)]
hoopoe$ring <- as.factor(hoopoe$ring)
```

### What about the data ??

#### Creation of the explicative variables

We don't forget to scale the variables.

```{r Définition variables explicatives, include = F}
# %bareground
x_bg <- hoopoe$bare
hoopoe$x_bg_carr <- (x_bg)^2
# vegetation height
x_vh <- hoopoe$heightC
hoopoe$x_vh_carr <- (x_vh)^2
```

We put the response variable as a numerical variable instead of a character one.

Doesn't seem to have a relation between the y variable (bird occurrence) and the two explicative variables ?? (not logical with their analysis though...)

#### Classic representations

```{r Some classic representation}
hoopoe$yi <- ifelse(hoopoe$y == "random", 0, 1)
hist(hoopoe$bare)
hist(hoopoe$heightC)
plot(hoopoe$bare, hoopoe$yi)
plot(hoopoe$heightC, hoopoe$yi)
```

## 2. Ajusting a simple model

Our goal is to find a relation between the %bareground or the vegetation height and the response variable (bird occurrence).

For that, we will use a Bayesian approach (since that the name of the course lol) while considering random effects on the slope and on the intercept. To construct that model, we need priors and data. Since we don't have any assumption available on the two explicative variables (actually yes we can imagine some with the representation but let's forget about that), we will consider a normal distribird occurrencebird occurrencebution for the priors, which are here not informative. So, one of the assumptions is that we have enough information in our data to represent the structure and the interaction that we are researching (annnnnnd we have 13 individuals.... robust analysis).

### Construction of the model

To construct the model, we need to regroup a lot of variables inside an only dataset.

The model take two things into account : the number of rows (N), the number of observations, and the number of individuals (nb), what we will consider as a possible random effet. Our model have one response variable (y), which is boolean (0 if the considered random loaction is random, 1 if it is not) and four explicative variables (%bareground, square(%bareground), vegetation height, square(vegetation height).

### Creation of the dataset

```{r Création dataset}
datahoopoe <- list(
  N = length(hoopoe$y),
  y = ifelse(hoopoe$y == "random", 0, 1),
  ring = as.numeric(hoopoe$ring),
  nb = length(levels(hoopoe$ring)),
  bare = as.numeric(scale(hoopoe$bare)),
  bare_carre = as.numeric(scale(hoopoe$x_bg_carr)),
  height = as.numeric(scale(hoopoe$heightC)),
  height_carre = as.numeric(scale(hoopoe$x_vh_carr))
)
```

### Construction of the function

To run the MCMC method, we need a function that will be put in input on Jags. That function takes into account random effect on the slopes and on the intercept. We then input the initial values of the parameters. We can finally run the model.

```{r Fonction que random effet sur a}
# Function
logisitic <- function(){
  for(i in 1:N){
    y[i] ~ dbin(p[i],1) # Binomial likelihood
    logit(p[i]) <- a[ring[i]] + b.bare * bare[i] + b.bare_carre * bare_carre[i] + b.height * height[i] 
    #+ b.height_carre* height_carre[i]
  }
  for (j in 1:nb){
    a[j] ~ dnorm(mua,tau.a) 
  }
b.bare~ dnorm(0,0.001) 
b.bare_carre~ dnorm(0,0.001) 
b.height ~ dnorm(0,0.001) 
b.height_carre ~ dnorm(0,0.001) 
mua ~ dnorm(0,0.001) 
tau.a <- 1 / (sigma.a * sigma.a)
sigma.a ~ dunif(0,100)
}

# Initial values
init1 <- list(a = rep(0.5, datahoopoe$nb), b.bare = 0.5, b.bare_carre = 0.5, b.height = 0.5, b.height_carre = 0.5)
init2 <- list(a = rep(-0.5, datahoopoe$nb), b.bare = -0.5, b.bare_carre = -0.5, b.height = -0.5, b.height_carre = -0.5)
init3 <- list(a = rep(0, datahoopoe$nb), b.bare = 0, b.bare_carre = 0, b.height = 0, b.height_carre = 0)
init4 <- list(a = rep(1, datahoopoe$nb), b.bare = 1, b.bare_carre = 1, b.height = 1, b.height_carre = 1)
init5 <- list(a = rep(-1, datahoopoe$nb), b.bare = -1, b.bare_carre = -1, b.height = -1, b.height_carre = -1)

init <- list(init1, init2, init3, init4, init5)

# Parameters to monitor
params <- c("a","b.bare", "b.bare_carre", "b.height", "b.height_carre")

# Call jags to fit model
partial_pooling_fit <- jags(data = datahoopoe,
                             inits = init,
                             parameters.to.save = params,
                             model.file = logisitic,
                             n.chains = 5,
                             n.iter = 40000,
                             n.burnin = 7500,
                             n.thin = 1)
partial_pooling_fit
```

```{r}
logisitic2 <- function(){
  for(i in 1:N){
    y[i] ~ dbin(p[i],1) # Binomial likelihood
    logit(p[i]) <- a[ring[i]] + b.bare[ring[i]] * bare[i] + b.bare_carre[ring[i]] * bare_carre[i] + b.height[ring[i]] * height[i] #+ b.height_carre[ring[i]] * height_carre[i]
  }
  for (j in 1:nb){
    a[j] ~ dnorm(mua,tau.a) 
    b.bare[j] ~ dnorm(mubare,tau.b)
    b.bare_carre[j] ~ dnorm(mubarec,tau.bc)
    b.height[j] ~ dnorm(muheight,tau.v)
    b.height_carre[j] ~ dnorm(muheightc,tau.vc) 
  }
mua ~ dnorm(0,0.001) 
mubare ~ dnorm(0,0.001) 
mubarec ~ dnorm(0,0.001) 
muheight ~ dnorm(0,0.001)
muheightc ~ dnorm(0,0.001)
tau.a <- 1 / (sigma.a * sigma.a)
sigma.a ~ dunif(0,100)
tau.b <- 1 / (sigma.b * sigma.b)
sigma.b ~ dunif(0,100)
tau.bc <- 1 / (sigma.bc * sigma.bc)
sigma.bc ~ dunif(0,100)
tau.v <- 1 / (sigma.v * sigma.v)
sigma.v ~ dunif(0,100)
tau.vc <- 1 / (sigma.vc * sigma.vc)
sigma.vc ~ dunif(0,100)
}

# Initial values
init1 <- list(a = rep(0.5, datahoopoe$nb), b.bare = rep(1, datahoopoe$nb), b.bare_carre = rep(1, datahoopoe$nb), b.height = rep(1, datahoopoe$nb), b.height_carre = rep(1, datahoopoe$nb))
init2 <- list(a = rep(-0.5, datahoopoe$nb), b.bare = rep(-1, datahoopoe$nb), b.bare_carre = rep(-1, datahoopoe$nb), b.height = rep(-1, datahoopoe$nb), b.height_carre = rep(-1, datahoopoe$nb))
init3 <- list(a = rep(0, datahoopoe$nb), b.bare = rep(0, datahoopoe$nb), b.bare_carre = rep(0, datahoopoe$nb), b.height = rep(0, datahoopoe$nb), b.height_carre = rep(0, datahoopoe$nb))
init4 <- list(a = rep(-1, datahoopoe$nb), b.bare = rep(-1, datahoopoe$nb), b.bare_carre = rep(-1, datahoopoe$nb), b.height = rep(-1, datahoopoe$nb), b.height_carre = rep(-1, datahoopoe$nb))
init5 <- list(a = rep(1, datahoopoe$nb), b.bare = rep(1, datahoopoe$nb), b.bare_carre = rep(1, datahoopoe$nb), b.height = rep(1, datahoopoe$nb), b.height_carre = rep(1, datahoopoe$nb))

init <- list(init1, init2, init3, init4, init5)

# Parameters to monitor
params <- c("a","b.bare", "b.bare_carre", "b.height", "b.height_carre")

# Call jags to fit model
partial_pooling_fit2 <- jags(data = datahoopoe,
                             inits = init,
                             parameters.to.save = params,
                             model.file = logisitic2,
                             n.chains = 5,
                             n.iter = 40000,
                             n.burnin = 7500,
                             n.thin = 1)
partial_pooling_fit2
```

```{r}
a_samples <- partial_pooling_fit2[["BUGSoutput"]][["mean"]][["a"]]
b_bare_samples <- partial_pooling_fit2[["BUGSoutput"]][["mean"]][["b.bare"]]
b_bare_carre_samples <- partial_pooling_fit2[["BUGSoutput"]][["mean"]][["b.bare_carre"]]
b_height_samples <- partial_pooling_fit2[["BUGSoutput"]][["mean"]][["b.height"]]
b_height_carre_samples <- partial_pooling_fit2[["BUGSoutput"]][["mean"]][["b.height_carre"]]

a_mean <- mean(a_samples)
b_bare_mean <- mean(b_bare_samples)
b_bare_carre_mean <- mean(b_bare_carre_samples)
b_height_mean <- mean(b_height_samples)
b_height_mean_carre <- mean(b_height_carre_samples)

bare_values <- hoopoe$bare
bare_values_carre <- bare_values^2
bare_values_scale <- scale(bare_values)
bare_values_carre_scale <- scale(bare_values_carre)
height <- hoopoe$heightC
height_scale <- scale(height)

logit_p <- a_mean + b_bare_mean*bare_values_scale + b_bare_carre_mean*bare_values_carre_scale + b_height_mean*height_scale

df <- data.frame(
  bare = hoopoe$bare,
  height = hoopoe$heightC,
  probability = p_mean
)

ggplot(df, aes(x = bare, y = probability)) +
  labs(title = "Probabilité en fonction de % Bareground",
    x = "% Bareground",
    y = "Probabilité") +
  theme_minimal() +
  scale_y_continuous(limits = c(0,1)) +
  scale_x_continuous(limits = c(0,100)) +
  geom_smooth(method = "gam", formula = y ~ poly(x,2), col = "lightblue", se = F)


ggplot(df, aes(x = height, y = probability)) +
  labs(title = "Probabilité en fonction de % Vegetation Height",
    x = "% Vegetation Height",
    y = "Probabilité") +
  theme_minimal() + geom_point() +
  scale_y_continuous(limits = c(0,1)) +
  scale_x_continuous(limits = c(0,40))
```

```{r}
data <- matrix(nrow = nrow(hoopoe), ncol = 8)
colnames(data) <- c("ring", "a", "b.bare", "b.bare_carre", "b.height", "b.height_carre", "bare", "height")
data[,1] <- hoopoe$ring
data[,7] <- hoopoe$bare
data[,8] <- hoopoe$heightC
for (i in 1:nrow(data)){
  data[i,2] <- a_samples[data[i,1]]
  data[i,3] <- b_bare_samples[data[i,1]]
  data[i,4] <- b_bare_carre_samples[data[i,1]]
  data[i,5] <- b_height_samples[data[i,1]]
  data[i,6] <- b_height_carre_samples[data[i,1]]
}
height <- scale(data[,8])
height_C <- scale(data[,8]^2)
bare <- scale(data[,7])
bare_C <- scale(data[,7]^2)
logit_p <- data[,2] + data[,3]*bare + data[,4]*bare_C + data[,5]*height

logit_p <- matrix(nrow = length(bare), ncol = 13)
p_mean_vecteur <- matrix(nrow = length(bare), ncol = 15)
p_mean_vecteur[,14] <- hoopoe$bare
p_mean_vecteur[,15] <- hoopoe$heightC
colnames(p_mean_vecteur) <- c("p1","p2","p3","p4","p5","p6","p7","p8","p9","p10","p11","p12","p13","bare", "height")

for(i in 1:13){
  logit_p[,i] <- a_samples[i] + b_bare_samples[i]*bare + b_bare_carre_samples[i]*bare_C + b_height_samples[i]*height
  p_mean_vecteur[,i] <- plogis(logit_p[,i])
}

df3 <- as.data.frame(p_mean_vecteur)
df3$p14 <- df$probability

ggplot(df3, aes(x = bare, y = p14)) +
  labs(title = "Probabilité en fonction de % Bareground",
    x = "% Bareground",
    y = "Probabilité") +
  theme_minimal() +
  scale_y_continuous(limits = c(0, 1)) +
  scale_x_continuous(limits = c(0, 100)) +
  geom_smooth(method = "gam", formula = y ~ poly(x,2), col = "lightblue", aes(y = p1), se = F)+
  geom_smooth(method = "gam", formula = y ~ poly(x,2), col = "lightblue", aes(y = p2), se = F)+
  geom_smooth(method = "gam", formula = y ~ poly(x,2), col = "lightblue", aes(y = p3), se = F)+
  geom_smooth(method = "gam", formula = y ~ poly(x,2), col = "lightblue", aes(y = p4), se = F)+
  geom_smooth(method = "gam", formula = y ~ poly(x,2), col = "lightblue", aes(y = p5), se = F)+
  geom_smooth(method = "gam", formula = y ~ poly(x,2), col = "lightblue", aes(y = p6), se = F)+
  geom_smooth(method = "gam", formula = y ~ poly(x,2), col = "lightblue", aes(y = p7), se = F)+
  geom_smooth(method = "gam", formula = y ~ poly(x,2), col = "lightblue", aes(y = p8), se = F)+
  geom_smooth(method = "gam", formula = y ~ poly(x,2), col = "lightblue", aes(y = p9), se = F)+
  geom_smooth(method = "gam", formula = y ~ poly(x,2), col = "lightblue", aes(y = p10), se = F)+ 
  geom_smooth(method = "gam", formula = y ~ poly(x,2), col = "lightblue", aes(y = p11), se = F)+
  geom_smooth(method = "gam", formula = y ~ poly(x,2), col = "lightblue", aes(y = p12), se = F)+
  geom_smooth(method = "gam", formula = y ~ poly(x,2), col = "lightblue", aes(y = p13), se = F)+
  geom_smooth(method = "gam", formula = y ~ poly(x,3), col = "black", aes(y = p14))

ggplot(df3, aes(x = height, y = p14)) +
  labs(title = "Probabilité en fonction de Vegetation Height",
    x = "Vegetation Height",
    y = "Probabilité") +
  theme_minimal() +
  scale_y_continuous(limits = c(0, 1)) +
  scale_x_continuous(limits = c(0, 40)) +
  geom_smooth(method = "gam", formula = y ~ poly(x,2), col = "lightblue", aes(y = p1), se = F)+
  geom_smooth(method = "gam", formula = y ~ poly(x,2), col = "lightblue", aes(y = p2), se = F)+
  geom_smooth(method = "gam", formula = y ~ poly(x,2), col = "lightblue", aes(y = p3), se = F)+
  geom_smooth(method = "gam", formula = y ~ poly(x,2), col = "lightblue", aes(y = p4), se = F)+
  geom_smooth(method = "gam", formula = y ~ poly(x,2), col = "lightblue", aes(y = p5), se = F)+
  geom_smooth(method = "gam", formula = y ~ poly(x,2), col = "lightblue", aes(y = p6), se = F)+
  geom_smooth(method = "gam", formula = y ~ poly(x,2), col = "lightblue", aes(y = p7), se = F)+
  geom_smooth(method = "gam", formula = y ~ poly(x,2), col = "lightblue", aes(y = p8), se = F)+
  geom_smooth(method = "gam", formula = y ~ poly(x,2), col = "lightblue", aes(y = p9), se = F)+
  geom_smooth(method = "gam", formula = y ~ poly(x,2), col = "lightblue", aes(y = p10), se = F)+ 
  geom_smooth(method = "gam", formula = y ~ poly(x,2), col = "lightblue", aes(y = p11), se = F)+
  geom_smooth(method = "gam", formula = y ~ poly(x,2), col = "lightblue", aes(y = p12), se = F)+
  geom_smooth(method = "gam", formula = y ~ poly(x,2), col = "lightblue", aes(y = p13), se = F)+
  geom_smooth(method = "gam", formula = y ~ poly(x,1), col = "black", aes(y = p14))
```

```{r Modèle fréquentiste}
library(lme4)
hoopoe$yi <- ifelse(hoopoe$y == "random", 0, 1)
hoopoe$bare <- scale(hoopoe$bare)
hoopoe$heightC <- scale(hoopoe$heightC)
complete_pooling_fit_mle <- glmer(yi ~ bare + (bare|ring) + (heightC|ring), data = hoopoe)
summary(complete_pooling_fit_mle)
```

## 3. Comparing the different models

Sooooo, like, what are we suppose to put here ???

We are gonna use the DIC to choose the best model, which is .....

## 4. Infering and interpreting the best model

Sur la base du meilleur modèle, donnez les estimations des paramètres ainsi qu'une mesure de l'incertitude associée. Interprétez vos résultats.

```{r}
traceplot(partial_pooling_fit)
```

With that, we can use some representations :

```{r}
invlogit(x) ##inverse logit
plot(proba(y), hoopoe$bareground)
# Refaire le graphe de l'article :)
```

## 5. Discussion

Comparez vos résultats à ceux du papier. Sont-ils semblables ou différents? Pourquoi selon vous? Si cela vous semble pertinent, proposez des pistes d'amélioration de l'analyse.

Well, here, we literally re use the same method as the article sooooooo....

maybe using an exponential or a poisson priors ??? like it's more informative ??? idk
